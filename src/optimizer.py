#!/usr/bin/env python3
"""
Token Optimizer - Main Optimization Module
Applies token optimization configurations to OpenClaw.
"""

import json
import os
import sys
import shutil
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
import argparse

# ANSI color codes
class Colors:
    RED = '\033[91m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    MAGENTA = '\033[95m'
    CYAN = '\033[96m'
    WHITE = '\033[97m'
    BOLD = '\033[1m'
    END = '\033[0m'

def colorize(text: str, color: str) -> str:
    if sys.stdout.isatty():
        return f"{color}{text}{Colors.END}"
    return text


class TokenOptimizer:
    """Applies token optimizations to OpenClaw configuration."""

    def __init__(self, dry_run: bool = False):
        self.dry_run = dry_run
        self.openclaw_dir = Path.home() / '.openclaw'
        self.config_path = self.openclaw_dir / 'openclaw.json'
        self.backup_dir = self.openclaw_dir / 'backups'
        self.templates_dir = Path(__file__).parent.parent / 'templates'

    def backup_config(self) -> Optional[Path]:
        """Create backup of existing configuration."""
        if not self.config_path.exists():
            return None

        self.backup_dir.mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_path = self.backup_dir / f'openclaw_{timestamp}.json'

        if not self.dry_run:
            shutil.copy(self.config_path, backup_path)
            print(colorize(f"[BACKUP] Config backed up to: {backup_path}", Colors.BLUE))
        else:
            print(colorize(f"[DRY-RUN] Would backup config to: {backup_path}", Colors.YELLOW))

        return backup_path

    def load_config(self) -> Dict:
        """Load existing config or return empty dict."""
        if self.config_path.exists():
            try:
                with open(self.config_path, 'r') as f:
                    return json.load(f)
            except json.JSONDecodeError:
                print(colorize("[WARNING] Existing config is invalid JSON, starting fresh", Colors.YELLOW))
        return {}

    def save_config(self, config: Dict):
        """Save configuration to file."""
        self.openclaw_dir.mkdir(parents=True, exist_ok=True)

        if self.dry_run:
            print(colorize("\n[DRY-RUN] Would save config:", Colors.YELLOW))
            print(json.dumps(config, indent=2))
        else:
            with open(self.config_path, 'w') as f:
                json.dump(config, f, indent=2)
            print(colorize(f"[SAVED] Config written to: {self.config_path}", Colors.GREEN))

    def generate_optimized_config(self) -> Dict:
        """Generate fully optimized OpenClaw configuration."""
        return {
            "agents": {
                "defaults": {
                    "model": {
                        "primary": "anthropic/claude-haiku-4-5"
                    },
                    "cache": {
                        "enabled": True,
                        "ttl": "5m",
                        "priority": "high"
                    },
                    "models": {
                        "anthropic/claude-sonnet-4-5": {
                            "alias": "sonnet",
                            "cache": True
                        },
                        "anthropic/claude-haiku-4-5": {
                            "alias": "haiku",
                            "cache": False
                        },
                        "anthropic/claude-opus-4-5": {
                            "alias": "opus",
                            "cache": True
                        }
                    }
                }
            },
            "heartbeat": {
                "every": "1h",
                "model": "ollama/llama3.2:3b",
                "session": "main",
                "target": "slack",
                "prompt": "Check: Any blockers, opportunities, or progress updates needed?"
            },
            "rate_limits": {
                "api_calls": {
                    "min_interval_seconds": 5,
                    "web_search_interval_seconds": 10,
                    "max_searches_per_batch": 5,
                    "batch_cooldown_seconds": 120
                }
            },
            "budgets": {
                "daily": 5.00,
                "monthly": 200.00,
                "warning_threshold": 0.75
            },
            "_meta": {
                "optimized_by": "token-optimizer",
                "version": "1.0.0",
                "optimized_at": datetime.now().isoformat()
            }
        }

    def merge_config(self, existing: Dict, optimized: Dict) -> Dict:
        """Merge optimized settings into existing config, preserving user customizations."""
        def deep_merge(base: Dict, override: Dict) -> Dict:
            result = base.copy()
            for key, value in override.items():
                if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                    result[key] = deep_merge(result[key], value)
                else:
                    result[key] = value
            return result

        return deep_merge(existing, optimized)

    def apply_model_routing(self, config: Dict) -> Dict:
        """Apply model routing optimization only."""
        optimized = self.generate_optimized_config()

        if 'agents' not in config:
            config['agents'] = {}
        if 'defaults' not in config['agents']:
            config['agents']['defaults'] = {}

        config['agents']['defaults']['model'] = optimized['agents']['defaults']['model']
        config['agents']['defaults']['models'] = optimized['agents']['defaults']['models']

        print(colorize("[APPLIED] Model routing: Haiku default, Sonnet/Opus aliases", Colors.GREEN))
        return config

    def apply_heartbeat(self, config: Dict) -> Dict:
        """Apply heartbeat optimization only."""
        optimized = self.generate_optimized_config()
        config['heartbeat'] = optimized['heartbeat']

        print(colorize("[APPLIED] Heartbeat: Ollama llama3.2:3b (free)", Colors.GREEN))
        return config

    def apply_caching(self, config: Dict) -> Dict:
        """Apply prompt caching optimization only."""
        optimized = self.generate_optimized_config()

        if 'agents' not in config:
            config['agents'] = {}
        if 'defaults' not in config['agents']:
            config['agents']['defaults'] = {}

        config['agents']['defaults']['cache'] = optimized['agents']['defaults']['cache']

        print(colorize("[APPLIED] Prompt caching: Enabled with 5m TTL", Colors.GREEN))
        return config

    def apply_rate_limits(self, config: Dict) -> Dict:
        """Apply rate limits and budgets."""
        optimized = self.generate_optimized_config()
        config['rate_limits'] = optimized['rate_limits']
        config['budgets'] = optimized['budgets']

        print(colorize("[APPLIED] Rate limits and budget controls", Colors.GREEN))
        return config

    def check_ollama(self) -> bool:
        """Check if Ollama is installed and running."""
        try:
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=5)
            return result.returncode == 0
        except (subprocess.SubprocessError, FileNotFoundError):
            return False

    def setup_ollama_heartbeat(self) -> bool:
        """Attempt to set up Ollama for heartbeat."""
        print(colorize("\n--- Setting up Ollama for Heartbeat ---", Colors.BOLD))

        if not self.check_ollama():
            print(colorize("[WARNING] Ollama not detected", Colors.YELLOW))
            print("  Install Ollama from: https://ollama.ai")
            print("  Then run: ollama pull llama3.2:3b")
            return False

        # Check if model is available
        try:
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
            if 'llama3.2:3b' not in result.stdout and 'llama3.2' not in result.stdout:
                print(colorize("[INFO] Pulling llama3.2:3b model...", Colors.CYAN))
                if not self.dry_run:
                    subprocess.run(['ollama', 'pull', 'llama3.2:3b'], check=True)
                print(colorize("[SUCCESS] Model ready", Colors.GREEN))
        except subprocess.SubprocessError as e:
            print(colorize(f"[ERROR] Failed to pull model: {e}", Colors.RED))
            return False

        return True

    def optimize_full(self):
        """Apply all optimizations."""
        print(colorize("\n=== Token Optimizer - Full Optimization ===\n", Colors.BOLD + Colors.CYAN))

        # Backup existing config
        self.backup_config()

        # Load existing config
        existing = self.load_config()

        # Generate and merge optimized config
        optimized = self.generate_optimized_config()
        final_config = self.merge_config(existing, optimized)

        # Apply all optimizations
        print(colorize("\nApplying optimizations:", Colors.BOLD))
        print(colorize("  [1/4] Model routing (Haiku default)", Colors.GREEN))
        print(colorize("  [2/4] Heartbeat to Ollama (free)", Colors.GREEN))
        print(colorize("  [3/4] Prompt caching (90% savings)", Colors.GREEN))
        print(colorize("  [4/4] Rate limits & budgets", Colors.GREEN))

        # Save config
        self.save_config(final_config)

        # Setup Ollama
        self.setup_ollama_heartbeat()

        # Generate workspace templates
        self.generate_workspace_templates()

        # Generate system prompt additions
        self.generate_system_prompts()

        print(colorize("\n=== Optimization Complete ===", Colors.BOLD + Colors.GREEN))
        print("\nNext steps:")
        print("  1. Review generated files in ~/.openclaw/")
        print("  2. Add system prompt rules from ~/.openclaw/prompts/")
        print("  3. Start Ollama: ollama serve")
        print("  4. Verify with: token-optimizer verify")

    def optimize_mode(self, mode: str):
        """Apply specific optimization mode."""
        self.backup_config()
        config = self.load_config()

        if mode == 'routing':
            config = self.apply_model_routing(config)
        elif mode == 'heartbeat':
            config = self.apply_heartbeat(config)
            self.setup_ollama_heartbeat()
        elif mode == 'caching':
            config = self.apply_caching(config)
        elif mode == 'limits':
            config = self.apply_rate_limits(config)
        elif mode == 'full':
            self.optimize_full()
            return
        else:
            print(colorize(f"[ERROR] Unknown mode: {mode}", Colors.RED))
            return

        self.save_config(config)

    def generate_workspace_templates(self):
        """Generate optimized workspace file templates."""
        workspace_dir = self.openclaw_dir / 'workspace'
        workspace_dir.mkdir(parents=True, exist_ok=True)

        # SOUL.md template
        soul_content = """# SOUL.md - Agent Core Principles

## Identity
[YOUR AGENT NAME/ROLE]

## Core Principles
1. Efficiency first - minimize token usage
2. Quality over quantity - precise responses
3. Proactive communication - surface blockers early

## How to Operate
- Default to Haiku for routine tasks
- Switch to Sonnet only for: architecture, security, complex reasoning
- Batch similar operations together
- Use memory_search() on demand, not auto-load

## Model Selection Rule
```
Default: Always use Haiku
Switch to Sonnet ONLY when:
- Architecture decisions
- Production code review
- Security analysis
- Complex debugging/reasoning
- Strategic multi-project decisions

When in doubt: Try Haiku first.
```

## Rate Limits
- 5s between API calls
- 10s between searches
- Max 5 searches/batch, then 2min break
"""

        # USER.md template
        user_content = """# USER.md - User Context

## Profile
- **Name:** [YOUR NAME]
- **Timezone:** [YOUR TIMEZONE]
- **Working Hours:** [YOUR HOURS]

## Mission
[WHAT YOU'RE BUILDING]

## Success Metrics
1. [METRIC 1]
2. [METRIC 2]
3. [METRIC 3]

## Communication Preferences
- Brief, actionable updates
- Surface blockers immediately
- Daily summary at end of session
"""

        # IDENTITY.md template
        identity_content = """# IDENTITY.md - Agent Identity

## Role
[AGENT ROLE - e.g., "Technical Lead", "Research Assistant"]

## Expertise
- [DOMAIN 1]
- [DOMAIN 2]
- [DOMAIN 3]

## Constraints
- Stay within defined budgets
- Follow rate limits strictly
- Escalate uncertainty early
"""

        templates = {
            'SOUL.md': soul_content,
            'USER.md': user_content,
            'IDENTITY.md': identity_content
        }

        print(colorize("\n--- Generating Workspace Templates ---", Colors.BOLD))

        for filename, content in templates.items():
            filepath = workspace_dir / filename
            if filepath.exists() and not self.dry_run:
                print(colorize(f"  [SKIP] {filename} already exists", Colors.YELLOW))
            else:
                if not self.dry_run:
                    with open(filepath, 'w') as f:
                        f.write(content.strip())
                print(colorize(f"  [CREATED] {filepath}", Colors.GREEN))

    def generate_system_prompts(self):
        """Generate system prompt additions for optimization."""
        prompts_dir = self.openclaw_dir / 'prompts'
        prompts_dir.mkdir(parents=True, exist_ok=True)

        # Session initialization rule
        session_init = """## SESSION INITIALIZATION RULE

On every session start:
1. Load ONLY these files:
   - SOUL.md
   - USER.md
   - IDENTITY.md
   - memory/YYYY-MM-DD.md (if it exists)

2. DO NOT auto-load:
   - MEMORY.md
   - Session history
   - Prior messages
   - Previous tool outputs

3. When user asks about prior context:
   - Use memory_search() on demand
   - Pull only the relevant snippet with memory_get()
   - Don't load the whole file

4. Update memory/YYYY-MM-DD.md at end of session with:
   - What you worked on
   - Decisions made
   - Leads generated
   - Blockers
   - Next steps

This saves 80% on context overhead.
"""

        # Model selection rule
        model_selection = """## MODEL SELECTION RULE

Default: Always use Haiku

Switch to Sonnet ONLY when:
- Architecture decisions
- Production code review
- Security analysis
- Complex debugging/reasoning
- Strategic multi-project decisions

When in doubt: Try Haiku first.
"""

        # Rate limits rule
        rate_limits = """## RATE LIMITS

- 5 seconds minimum between API calls
- 10 seconds between web searches
- Max 5 searches per batch, then 2-minute break
- Batch similar work (one request for 10 leads, not 10 requests)
- If you hit 429 error: STOP, wait 5 minutes, retry

## DAILY BUDGET: $5 (warning at 75%)
## MONTHLY BUDGET: $200 (warning at 75%)
"""

        # Combined optimization prompt
        combined = f"""# TOKEN OPTIMIZATION RULES

Add these rules to your agent's system prompt:

---

{session_init}

---

{model_selection}

---

{rate_limits}

---

## IMPORTANT
These rules work together to reduce costs by 97%.
Do not remove or modify unless you understand the cost implications.
"""

        prompts = {
            'session-init.md': session_init,
            'model-selection.md': model_selection,
            'rate-limits.md': rate_limits,
            'OPTIMIZATION-RULES.md': combined
        }

        print(colorize("\n--- Generating System Prompts ---", Colors.BOLD))

        for filename, content in prompts.items():
            filepath = prompts_dir / filename
            if not self.dry_run:
                with open(filepath, 'w') as f:
                    f.write(content.strip())
            print(colorize(f"  [CREATED] {filepath}", Colors.GREEN))

        print(colorize(f"\n[INFO] Add contents of {prompts_dir / 'OPTIMIZATION-RULES.md'} to your system prompt", Colors.CYAN))


def main():
    parser = argparse.ArgumentParser(description='Token Optimizer for OpenClaw')
    parser.add_argument('--mode', choices=['full', 'routing', 'heartbeat', 'caching', 'limits'],
                       default='full', help='Optimization mode')
    parser.add_argument('--dry-run', action='store_true', help='Show what would be done without making changes')

    args = parser.parse_args()

    optimizer = TokenOptimizer(dry_run=args.dry_run)
    optimizer.optimize_mode(args.mode)

    return 0


if __name__ == '__main__':
    sys.exit(main())
